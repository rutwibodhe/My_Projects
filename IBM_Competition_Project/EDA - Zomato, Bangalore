# -*- coding: utf-8 -*-
"""Ibm_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YT-JKObQJZR2LGZdgaVxbfHNiTzMKMBG
"""

from google.colab import drive

drive.mount('/content/gdrive')

# Reading required libraries
import pandas as pd
import numpy as np
import seaborn as sn
import matplotlib.pyplot as plt
import plotly.offline as py
import plotly.graph_objs  as go
from IPython.display import display
from plotly.offline import init_notebook_mode
init_notebook_mode(connected=False)
from wordcloud import WordCloud
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor

# Reading the dataset
df = pd.read_csv('/content/gdrive/My Drive/zomato.csv')
df.head()

#studying the data
df.info()

df.rename(columns = {'rate':'rating'},inplace=True)

df.drop(['url','phone','reviews_list'],axis=1,inplace=True)

df.duplicated().sum()
df.drop_duplicates(inplace = True)

df.isnull().sum()

df.isnull().sum()
df.dropna(how = 'any',inplace=True)
df.info()

df.rename(columns = {'listed_in(type)':'type','listed_in(city)':'city'},inplace=True)

df.columns

df.rename(columns = {'approx_cost(for two people)':'cost'},inplace=True)

df.columns

#Examinig Cost column
df['cost'].unique()

df['cost'] = df['cost'].astype(str)
df['cost'] = df['cost'].apply(lambda x:x.replace(',',''))
df['cost'] = df['cost'].astype(float)
df.info()

df['cost'].unique()

df['rating'].unique()

df = df.loc[df.rating != 'NEW']
df = df.loc[df.rating != '-']
clean_slash = lambda x: x .replace('/5','') if type(x)==np.str else x
df.rating = df.rating.apply(clean_slash).str.strip().astype('float')
df['rating'].head()

df.name = df.name.apply(lambda x:x.title())
df.online_order.replace(('Yes','No'),(True,False),inplace = True)
df.book_table.replace(('Yes','No'),(True,False),inplace = True)
df.head(2)

#Checking The Feasibility Of Data
def Encode(df):
  for col in df.columns[~df.columns.isin(['rating','cost','votes'])]:
    df[col] = df[col].factorize()[0]
  return df

data_en = Encode(df.copy())

#Plotting the Co-relation Graph
plt.figure(figsize = (18,10))
corr = data_en.corr(method = 'kendall')
sn.heatmap(corr,annot=True)

#Applying Test and Train data
x = data_en.iloc[:,[2,3,5,6,7,8,9,11]]
y = data_en['rating']
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=.1,random_state=300)
x_train.head()
y_train.head()

#compatiblity of Linear model
linear = LinearRegression()
linear.fit(x_train,y_train)
y_pred = linear.predict(x_test)
y_pred

#Linear model variance(r2_score)
r2_score(y_test,y_pred)

#r2 score is very low means higah variance using linear regression

#compatibility of decision tree
rforest = RandomForestRegressor(n_estimators=500,random_state=300,min_samples_leaf=.0001)
rforest.fit(x_train,y_train)
y_predict = rforest.predict(x_test)
y_predict

#forest regressor r2 score
r2_score(y_test,y_predict)

# we can use forest regressor

plt.figure(figsize=(10,7))
chains=df['name'].value_counts()[:20]
sn.barplot(x=chains,y=chains.index,palette='deep')
plt.title("Most famous restaurants chains in Bangaluru")
plt.xlabel("Number of outlets")

sn.countplot(df['city'])
sn.countplot(df['city']).set_xticklabels(sn.countplot(df['city']).get_xticklabels(),rotation = 90)
plt.gcf().set_size_inches(10,10)
plt.title('Location Count')

plt.figure(figsize=(20,5))
sn.distplot(df['rating'])
plt.title('Rate Distribution', fontsize=25)
plt.xlabel('Rate', fontsize=20)
plt.xticks(

        fontweight='light',
        fontsize='x-large'
    )
plt.show()

plt.figure(figsize=(10,7))
sn.scatterplot(x="rating",y='cost',hue='online_order',data=df)
plt.show()

sn.countplot(df['online_order'])
fig = plt.gcf()
fig.set_size_inches(5,5)
plt.title('Delivering Online or not')

sn.countplot(df['book_table'])
fig = plt.gcf()
fig.set_size_inches(5,5)
plt.title('Table Booking')

plt.figure(figsize=(12,6))

name = df['location'].value_counts()[:8].index
value = df['location'].value_counts()[:8].values
explode = (0.1,0,0,0,0,0,0,0)
plt.pie(value,explode=explode,labels=name,autopct='%1.1f%%',startangle=120)
plt.axis('equal')
plt.title('Percentage Of unique Restaurants in specific Location',color='r')
plt.show

type_pl = pd.crosstab(df['rating'],df['city'])
type_pl.plot(kind='bar',stacked=True)
plt.title('Location v/s Rating')
plt.xlabel('Rating')
plt.ylabel('Location')
plt.xticks(size = 10)
plt.legend().remove()

sn.countplot(df['type'])
sn.countplot(df['type']).set_xticklabels(sn.countplot(df['type']).get_xticklabels(), rotation=90, ha="right")
fig = plt.gcf()
fig.set_size_inches(5,5)
plt.title('Type of Dine in')

type_p = pd.crosstab(df['rating'],df['type'])
type_p.plot(kind='bar',stacked=True);
plt.title('Type v/s Rating')
plt.xlabel('Rating')
plt.ylabel('Type')
plt.xticks(size=10)
plt.yticks(size=10)
plt.figure(figsize=(15,15))
plt.show()

sn.countplot(df['cost'])
sn.countplot(df['cost']).set_xticklabels(sn.countplot(df['cost']).get_xticklabels(), rotation=90, ha="right")
fig = plt.gcf()
fig.set_size_inches(8,8)
plt.title("Approximate Cost of Restaurant")
plt.xticks(size=6)

df['dish_liked'] = df['dish_liked'].apply(lambda x:x.split(',') if type(x)==str else [''])
rest = df['rest_type'].value_counts()[:9].index
def Word(rest):
  plt.figure(figsize=(20,30))
  for i,r in enumerate(rest):
    plt.subplot(3,3,i+1)
    corpus = df[df['rest_type']==r]['dish_liked'].values.tolist()
    corpus = ','.join(x for list_words in corpus for x in list_words)
    wordcloud = WordCloud(max_font_size=None,background_color='red',collocations=False,
                          width=1200,height=1200).generate(corpus)
    plt.imshow(wordcloud)
    plt.title(r)
    plt.axis('off')
Word(rest)

"""# **7.Findings**"""

df = df.drop_duplicates('name',keep='first')
n_df = df[['name','cost','location','rest_type','cuisines']].groupby(['cost'],sort=True)
n_df = n_df.filter(lambda x: x.mean() <= 500)
n_df.sort_values(by=['cost'],inplace=True)

n_df_exp = df[['name','cost','location','rest_type','cuisines']].groupby(['cost'],sort=True)
n_df_exp = n_df_exp.filter(lambda x:x.mean()>=900)
n_df_exp = n_df_exp.sort_values(by = ['cost'])

"""n_df_exp is the list of expensive restaurants

"""

n_df_rat = df[['name','rating']].groupby(['rating'],sort=True)
n_df_rat = n_df_rat.filter(lambda x: x.mean() > 4)
df.rating.value_counts()
df.rating.unique()

"""n_df_rat is the list of highly-rated restaurants"""

Most_edible_Restaurants = pd.merge(n_df,n_df_rat,how='inner',on=['name'])
print('Affordable restaurants which are higly rated')
Most_edible_Restaurants.head()

Expensive_Restaurants = pd.merge(n_df_exp,n_df_rat,how='inner',on=['name'])
Expensive_Restaurants.head()

n_df.head()

n_df_rat.head()
